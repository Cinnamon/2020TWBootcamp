{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, BertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "config = AutoConfig.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\", use_fast=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey. Are you free on Tuesday? Yes. Do you want...</td>\n",
       "      <td>[[15, 18], [7, 7], [0, 0], [29, 31]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey. Are you free on Friday? Sorry, I have pla...</td>\n",
       "      <td>[[0, 0], [0, 0], [0, 0], [0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sup! Sup. Would you want to go rafting on Satu...</td>\n",
       "      <td>[[11, 14], [16, 16], [29, 30], [41, 43]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yo, what's up? Nothing much. Let's watch a mov...</td>\n",
       "      <td>[[14, 16], [18, 18], [42, 43], [65, 67]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yo, what's up? Nothing much. Do you want to gr...</td>\n",
       "      <td>[[15, 16], [18, 18], [26, 27], [35, 36]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Hey. Are you free on Tuesday? Yes. Do you want...   \n",
       "1  Hey. Are you free on Friday? Sorry, I have pla...   \n",
       "2  Sup! Sup. Would you want to go rafting on Satu...   \n",
       "3  Yo, what's up? Nothing much. Let's watch a mov...   \n",
       "4  Yo, what's up? Nothing much. Do you want to gr...   \n",
       "\n",
       "                                      Label  \n",
       "0      [[15, 18], [7, 7], [0, 0], [29, 31]]  \n",
       "1          [[0, 0], [0, 0], [0, 0], [0, 0]]  \n",
       "2  [[11, 14], [16, 16], [29, 30], [41, 43]]  \n",
       "3  [[14, 16], [18, 18], [42, 43], [65, 67]]  \n",
       "4  [[15, 16], [18, 18], [26, 27], [35, 36]]  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SashimiDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(SashimiDataset, self).__init__()\n",
    "        self.text = data.Text\n",
    "        self.label = data.Label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.text.shape[0]*4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx // len(QUESTIONS)]\n",
    "        qs_idx = idx % len(QUESTIONS)\n",
    "        label = self.label[idx // len(QUESTIONS)][qs_idx]\n",
    "        question = QUESTIONS[qs_idx]\n",
    "        \n",
    "        inputs = tokenizer(question, text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        label = torch.LongTensor(label)\n",
    "        \n",
    "        plus_idx = (inputs['input_ids'] == 102).nonzero()[0,1]\n",
    "        if label.tolist() != [0,0]:\n",
    "            label += plus_idx \n",
    "        \n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "QUESTIONS = [\n",
    "    'What activity?',\n",
    "    'Which day?',\n",
    "    'What time on that day?',\n",
    "    'Which place to go?'\n",
    "]\n",
    "\n",
    "data = pd.read_csv('./data/updated3500s.csv', sep='\\t')\n",
    "#data['Text'] = [ast.literal_eval(data.Text[i])[0] for i in range(data.shape[0])]\n",
    "data['Label'] = [ast.literal_eval(data.Label[i]) for i in range(data.shape[0])]\n",
    "    \n",
    "dataset = SashimiDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6507027b7f7445bd8b2e0a23cb36608b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True:    [[19, 22], [11, 11], [0, 0], [35, 37]]\n",
      "Predict: [[20, 22], [11, 11], [14, 14], [22, 24]]\n",
      "tensor([19, 22]) go rafting \n",
      "\n",
      "tensor([11, 11]) Tuesday \n",
      "\n",
      "tensor([0, 0]) [CLS] \n",
      "\n",
      "tensor([35, 37]) park? OK \n",
      "\n",
      "True:    [[0, 0], [0, 0], [0, 0], [0, 0]]\n",
      "Predict: [[0, 0], [23, 23], [26, 26], [25, 25]]\n",
      "tensor([0, 0]) [CLS] \n",
      "\n",
      "tensor([0, 0]) [CLS] \n",
      "\n",
      "tensor([0, 0]) [CLS] \n",
      "\n",
      "tensor([0, 0]) [CLS] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (inputs, label) in tqdm.notebook.tqdm(enumerate(dataloader)):\n",
    "    for k in inputs.keys():\n",
    "        inputs[k] = inputs[k].squeeze(1)\n",
    "    outputs = model(**inputs)\n",
    "    start, end = torch.argmax(outputs[0], -1), torch.argmax(outputs[1], -1)\n",
    "    print(f\"True:    {label.tolist()}\\nPredict: {torch.stack((start, end)).T.tolist()}\")\n",
    "    for lb in label:\n",
    "        print(lb, tokenizer.decode(inputs['input_ids'][0, lb[0]:lb[1]+1].tolist()), '\\n')\n",
    "    if idx == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey. Are you free on Tuesday? Yes. Do you want to go rafting? Sure! Where should we meet? How about the theme park? OK! See you there!'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 12])\n",
      "tensor(4) tensor([15, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/8000 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, BertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class SashimiDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(SashimiDataset, self).__init__()\n",
    "        self.text = data.Text\n",
    "        self.label = data.Label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.text.shape[0]*4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx // len(QUESTIONS)]\n",
    "        qs_idx = idx % len(QUESTIONS)\n",
    "        label = self.label[idx // len(QUESTIONS)][qs_idx]\n",
    "        question = QUESTIONS[qs_idx]\n",
    "        \n",
    "        inputs = tokenizer(question, text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        label = torch.LongTensor(label)\n",
    "        \n",
    "        print(label)\n",
    "        \n",
    "        plus_idx = (inputs['input_ids'] == 102).nonzero()[0,1]\n",
    "        label += plus_idx + 1\n",
    "        \n",
    "        print(plus_idx, label)\n",
    "        \n",
    "        return inputs, label\n",
    "        \n",
    "        \n",
    "def get_data(path, sep=',', index_col=None):\n",
    "    data = pd.read_csv(path, sep=sep, index_col=index_col)\n",
    "    data['Text'] = [ast.literal_eval(data.Text[i])[0] for i in range(data.shape[0])]\n",
    "    data['Label'] = [ast.literal_eval(data.Label[i]) for i in range(data.shape[0])]\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "    model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\", use_fast=True)\n",
    "    \n",
    "    QUESTIONS = [\n",
    "        'What activity?',\n",
    "        'What date?',\n",
    "        'What time?',\n",
    "        'Where to go?'\n",
    "    ]\n",
    "\n",
    "    data = get_data('./data/training_data.csv', sep='\\t')\n",
    "    dataset = SashimiDataset(data)\n",
    "    dataloader = DataLoader(dataset, batch_size=1)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), 2e-5)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for inputs, label in tqdm.tqdm(dataloader):\n",
    "            # Reform the inputs\n",
    "            for k in inputs.keys():\n",
    "                inputs[k] = inputs[k].squeeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            start, end = outputs[0], outputs[1]\n",
    "            label = label\n",
    "\n",
    "            loss = loss_fn(start, label[:,0]) + loss_fn(end, label[:,1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            break\n",
    "            \n",
    "        break\n",
    "            \n",
    "        print(f'Train loss {total_loss}')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\t18\ta play on Friday\n"
     ]
    }
   ],
   "source": [
    "start, end = torch.argmax(outputs[0]), torch.argmax(outputs[1])\n",
    "print(f\"{start}\\t{end}\\t{tokenizer.decode(inputs['input_ids'][0,start:end+1].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sup! Sup. Would you want to go to a party tomorrow? Sure. Where should we meet? I don't know. How about the Japanese place? Great. When? You free 4 am? Sorry, I can't. How about 7 am? Ok. See you then!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'][1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
